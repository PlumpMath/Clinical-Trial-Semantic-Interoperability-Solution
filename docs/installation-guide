1. REQUIREMENTS

- These instructions are made to deploy the platform over Apache Tomcat-7.0.52 and java jdk1.7.0_75 on UNIX.

1.1 DATABASE
	
sudo apt-get install mysql-server mysql-common mysql-client
sudo apt-get install mysql-workbench

This example is based on a database with the following connection parameters:

	Hostname = 127.0.0.1
	Port = 3306
	Username = testCV
	Password = testCV

To create the user testCV and provide privileges execute create_user.sql on your localhost connection.

Whether a different connection is used, these parameters should be modify on all properties files of every service and generate new war files.

1.2 Java and tomcat configuration

It is needed to increase the java heap memory on catalina.sh (/usr/share/tomcat7/bin)

	CATALINA_OPTS="-Xms512M -Xmx2048M"

1.3. Mirth Connect

Mirth Connect is needed in order to do the ETL process to store HL7 v3 messages on databases.

- Download Mirth Connect for Linux (RPM) http://www.mirthcorp.com/community/downloads?form=false
- Extract RPM file on your personal directory
- Go to mirth connect conf directory (cd ~/opt/mirthconnect/conf)
- Modify mirth.properties (sudo nano mirth.properties)

        ...
	# ports
	http.port = 8081
	https.port = 8444
	...

- Save changes.
- Modify mirth-cli-config.properties (sudo nano mirth-cli-config.properties)

	address=https://127.0.0.1:8444
	...

- Save changes.
- Go to mirth connect folder (cd ~/opt/mirthconnect)
- Init command line interface (java -jar mirth-cli-launcher.jar -a https://127.0.0.1:8444 -u admin -p admin) 
- Import channel MirthConnectWS.xml (import ["path to file"]/MirthConnectWS.xml)
- Quit command line interface (QUIT)
- Init mirth connect service from mirth connect directory (sudo ./mcservice start)

The ETL process is now deployed and ready to be used by the Data Push Service.
Data Push Service has a stub to this url http://localhost:8091/services/MirthConnectWS. So it is important not to change name and port.

2. CREATE CDM

Create two new schemas on your SQL server. One called “testcv” and the other called “normalized_testcv”.

Execute file “dump_testCV.sql” on “testCV” database and “dump_normalized_testCV.sql” on “normalized_ testCV”. The first file creates a non-normalized database, and the second one creates a normalized one.

Create a new schema on your SQL server called “translations” and execute “translations.sql” over it. This database is used by Data Push Service.

Create a new schema on your SQL server called “normalization_errors” and execute “normalization_errors.sql”. This database is used by Data Push Service.

3. CoreDataset service deployment and generation

First of all, it is necessary to obtain an owl version of SNOMED-CT.
	
Ontology creation step by step (perl & java are required):
It is facilitated a script (jar) for creating the owl file.

	1. Get the SNOMED CT release. 
	a. sct2_Concept_Snapshot_INT_20140131.txt not provided
	b. sct2_Description_Snapshot-en_INT_20140131.txt not provided
	c. sct2_Relationship_Snapshot_INT_20140131.txt not provided
	2. Executed the script (ontologyCreation.jar).
		Some files have to be on the same folder of the jar:
		modScriptRF2IncludingNewPropertyProvenanceObtainingFromFileLoincTerminfo.pl
		terminfoJSON.txt
		treeTop.txt
		Arguments of the jar:
			arg1: sct2_Concept_Snapshot_INT_20140131.txt not provided
			arg2: sct2_Description_Snapshot-en_INT_20140131.txt not provided
			arg3: sct2_Relationship_Snapshot_INT_20140131.txt not provided
			arg4: LOINC.TXT provided
			arg5: HGNC.TXT provided
			arg6: snomed.owl output name of the file and location
	3. Execute (java –jar ontologyCreation.jar arg1 arg2….)

The resulting file has to be stored and named as follows:
OWL FILE: /var/lib/tomcat7/files/snomed.owl

Put the war file (CoreDatasetService.war) into the webapps directory (var/lib/tomcat7/webapps) with the correct permissions, tomcat7 user and RW flags. 
chown tomcat7:tomcat7 *
The deployment of this service will take about 4 minutes in machine with 4 gb RAM.
If the process works it generates:
http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

4. Query Execution service deployment

Query Execution service allows querying data against normalized database.
Copy “QueryExecutionService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.
Query Execution service provides a method to perform queries called “executeQuery”, which receives a string query inside an object from class “ExecuteQuery”. It is executed on “normalized_testcv” database.
The result set is a string encoded as xml. 
The war file is set up to make queries against “normalized_testcv. To change the database queried, the file “properties.config” (./webapps/ eureca_UDS/config/properties.config) should be modified. This file is contained on “QueryExecutionService.war” folder, inside “WebContent/config/”.
The following fields should be exchanged with the new connection parameters and the new database
 
	name:server=127.0.0.1
	database=normalized_testcv
	user_db=testCV
	pass_db=testCV

Once the properties file is changed, a new war file should be generated and copy on tomcat webapps folder.
If the process works it generates:
http://localhost:8080/QUeryExecutionService/services/SemanticInteroperabilityLayer?wsdl

NOTE: This service contains a stub to CoreDataset Service on this url: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

5. Query Builder service deployment

Query Builder service allows build queries to retrieve data related with SNOMED-CT concepts received as parameters.  
Copy “QueryBuilderService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.

If the process works it generates:
http:// localhost:8080/QueryBuilderService/services/QueryBuilderService?wsdl

NOTE: This service contains a stub to CoreDataset Service on this url: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

6. Data Push Service deployment

Data Push service allows storing new data on databases using HL7 v3 messages, in addition, the service provides a method to execute a dump file and recover the databases.

Copy “DataPushService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.

Data Push service provides two method:

- storeData

Stores new data from HL7 v3 messages. The data are stored without normalization in one database, and normalized in another one.

- erase_Data

Erase both databases, normalized and non-normalized executing “dump_normalized_testCV.sql” and “dump_testCV.sql” files.

The war file is set up to store data on “testCV” and “normalized_testCV” database. To change the databases, the files “properties.config” and “propertiesNormalization.config” should be modified. This file is contained on “DataPushService” folder, inside “WebContent/config/”.

The following fields should be exchanged with the new connection parameters and the new database:

- properties.config

	server=localhost
	database=testcv
	user_db=testCV
	pass_db=testCV 
	host_db=jdbc:mysql://127.0.0.1:3306/?autoReconnect=true 

- propertiesNormalization.config

	host_db=jdbc:mysql://127.0.0.1:3306/?autoReconnect=true
	host_db2=127.0.0.1
	user_db=testCV
	pass_db=testCV

If the process works it generates:

http://localhost:8080/DataPushService/services/DataPushService?wsdl

NOTE: This service contains a stub to CoreDataset Service and MirhtConnect on these urls: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl
http://localhost:8091/services/MirthConnectWS
