1. REQUIREMENTS

- These instructions are made to deploy the platform over Apache Tomcat7 and java 7 on UNIX.

1.1 DATABASE
	
sudo apt-get install mysql-server mysql-common mysql-client

This example is based on a database with the following connection parameters:

	Hostname = 127.0.0.1
	Port = 3306
	Username = testCV
	Password = testCV

To create the user testCV and provide privileges execute create_user.sql on your localhost connection. Located on /bin/sql

mysql -u USER -p
source /PATH_TO/create_user.sql

Whether a different connection is used, these parameters should be modify on all properties files of every service and generate new war files.

1.2 Java and tomcat configuration
1.2.1 Installation of Tomcat7 
Install tomcat7  with all the dependencies (it will install openjdk but we will change after)
	sudo apt-get install tomcat7
	
1.2.2 Installation and configuration of JAVA JDK
Now install Java 7
	sudo apt-get install python-software-properties
	sudo add-apt-repository ppa:webupd8team/java
	sudo apt-get update
	sudo apt-get install oracle-java7-installer

Let's see the installed java versions
	sudo update-alternatives --config java

Change the java version used in Tomcat7
	sudo vi /etc/default/tomcat7
Replace the variable JAVA_HOME with the java 7 path. In the example 
JAVA_HOME=/usr/lib/jvm/java-7-oracle

It is needed to increase the java heap memory on catalina.sh (/usr/share/tomcat7/bin/catalina.sh)

Add the variable CATALINA_OPTS on catalina.sh and then restart the machine
	CATALINA_OPTS="-Xms512M -Xmx2048M"

1.3. Mirth Connect

Mirth Connect is needed in order to do the ETL process to store HL7 v3 messages on databases.

- Download Mirth Connect for Linux (wget http://downloads.mirthcorp.com/connect/3.2.1.7650.b40/mirthconnect-3.2.1.7650.b40-unix.sh)
- Change file permissions (sudo chmod a+x mirthconnect-3.2.1.7650.b40-unix.sh)
- Install mirth connect (sudo bash mirthconnect-3.2.1.7650.b40-unix.sh)
	- Select defaults options until Network Ports where the port should be changed to:

		...
		Administrator Launcher Port
		8081
		Administrator Port
		8444
		...

	- Path to application data should be: /usr/local/mirthconnect/appdata
	- Path to logs should be: /usr/local/mirthconnect/logs
	- Finish installation and select Run Mirth Connect Service Manager
- Go to mirth connect directory (cd /usr/local/mirthconnect)
- Init command line interface (java -jar mirth-cli-launcher.jar -a https://127.0.0.1:8444 -u admin -p admin) 
- Import channel MirthConnectWS.xml (import ["path to file"]/MirthConnectWS.xml)
- Deploy channel (deploy)
- Quit command line interface (QUIT)
- Init mirth connect service from mirth connect directory (sudo ./mcservice start)

The ETL process is now deployed and ready to be used by the Data Push Service.
Data Push Service has a stub to this url http://localhost:8091/services/MirthConnectWS. So it is important not to change name and port.

Whenever the Data Push Service is used Mirth Connect should be running.  

2. UMLS-based Terminology Service

Terminology Service is needed in order to translate concepts which are not into the Core Dataset Service. 

Execute file "metathesaurus_schema.sql" on mysql server.
	SOURCE metathesaurus_schema.sql

Download UMLS from http://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html (Active release files).

Once UMLS is installed, a file called "MRCONSO.RRF" will be into the folder "2014AB/META" created by UMLS installation.

In file "load_metathesaurus.sql", change %%PATH_TO_MRCONSO%% to your path to "MRCONSO.RRF".

Execute file "load_metathesaurus.sql" on "metathesaurus" database on your SQL server. This will load the "MRCONSO" table with "MRCONSO.RRF" file.

Whether the error "Error Code: 1148. The used command is not allowed with this MySQL version" is got from the execution of load_metathesaurus, it is needed accessing mysql server with option --local-infile "mysql -u testCV -p --local-infile metathesaurus".

Put the war file (TerminologyService.war) into the webapps directory (var/lib/tomcat7/webapps) with the correct permissions, tomcat7 user and RW flags. 
If the process works it generates:
http://localhost:8080/TerminologyService/services/TerminologyService?wsdl 
 
 
3. CREATE CDM

Execute file “dump_testCV.sql” and “dump_normalized_testCV.sql” on mysql server. The first file creates a non-normalized database, and the second one creates a normalized one.
	SOURCE dump_testCV.sql;
	SOURCE dump_normalized_testCV.sql;


Execute file "testCV_deleted.sql" and "normalized_testCV_deleted.sql" on mysql server. Both databases are used to store deleted clinical acts due to overwriting. 
	SOURCE testCV_deleted.sql;
	SOURCE normalized_testCV_deleted.sql;
	
Execute “translations.sql” on mysql server. This database is used by Data Push Service.
	SOURCE translations.sql;

Execute “normalization_errors.sql”. This database is used by Data Push Service.
	SOURCE normalization_errors.sql;

4. CoreDataset service deployment and generation

First of all, it is necessary to obtain an owl version of SNOMED-CT.
	
Ontology creation step by step:
It is facilitated a java class on CoreDataset service for creating the owl file.

	1. Get the SNOMED CT release. 
		a. sct2_Concept_Snapshot_INT_20140131.txt not provided
		b. sct2_Description_Snapshot-en_INT_20140131.txt not provided
		c. sct2_Relationship_Snapshot_INT_20140131.txt not provided
	2. Get the LOINC release.
		a. LOINC.txt not provided
	3. Get the HGNC release.
		a. HGNC.txt not provided
	4. Execute the main method in TermBindingLoader3 class.
		Arguments of the main:
			arg1: sct2_Concept_Snapshot_INT_20140131.txt location
			arg2: sct2_Description_Snapshot-en_INT_20140131.txt location
			arg3: sct2_Relationship_Snapshot_INT_20140131.txt location
			arg4: LOINC.txt location
			arg5: HGNC.txt location
			arg6: snomed.owl output name of the file and location

The resulting file has to be stored and named as follows:
OWL FILE: /var/lib/tomcat7/files/snomed.owl

Put the war file (CoreDatasetService.war) into the webapps directory (var/lib/tomcat7/webapps) with the correct permissions, tomcat7 user and RW flags. 
chown tomcat7:tomcat7 *
The deployment of this service will take about 4 minutes in machine with 4 gb RAM.
If the process works it generates:
http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

5. Query Execution service deployment

Query Execution service allows querying data against normalized database.
Copy “QueryExecutionService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.
Query Execution service provides a method to perform queries called “executeQuery”, which receives a string query inside an object from class “ExecuteQuery”. It is executed on “normalized_testcv” database.
The result set is a string encoded as xml. 
The war file is set up to make queries against “normalized_testcv. To change the database queried, the file “properties.config” (./webapps/ eureca_UDS/config/properties.config) should be modified. This file is contained on “QueryExecutionService.war” folder, inside “WebContent/config/”.
The following fields should be exchanged with the new connection parameters and the new database
 
	name:server=127.0.0.1
	database=normalized_testcv
	user_db=testCV
	pass_db=testCV

Once the properties file is changed, a new war file should be generated and copy on tomcat webapps folder.
If the process works it generates:
http://localhost:8080/QUeryExecutionService/services/SemanticInteroperabilityLayer?wsdl

NOTE: This service contains a stub to CoreDataset Service on this url: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

6. Query Builder service deployment

Query Builder service allows build queries to retrieve data related with SNOMED-CT concepts received as parameters.  
Copy “QueryBuilderService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.

If the process works it generates:
http:// localhost:8080/QueryBuilderService/services/QueryBuilderService?wsdl

NOTE: This service contains a stub to CoreDataset Service on this url: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl

7. Data Push Service deployment

Data Push service allows storing new data on databases using HL7 v3 messages. In addition, the service provides a method to execute a dump file and recover the databases and a method rollback which allows undo changes to a certain given date.

Copy “DataPushService.war” file on your tomcat webapps folder and restart tomcat with the correct permissions, tomcat7 user and RW flags.
Use the wsdl file to generate a service client.

Data Push service provides three methods:

- storeData

Stores new data from HL7 v3 messages. The data are stored without normalization in one database, and normalized in another one.

- erase_Data

Erase both databases, normalized and non-normalized executing “dump_normalized_testCV.sql” and “dump_testCV.sql” files.

- rollback

Undo changes to a certain date using "testcv_deleted" and "normalized_testcv_deleted" databases. Both databases store messages which have been reloaded using method storeData into "testcv" and "normalized_testcv" databases.

The war file is set up to store data on “testCV” and “normalized_testCV” database. To change the databases, the files “properties.config” and “propertiesNormalization.config” should be modified. This file is contained on “DataPushService” folder, inside “WebContent/config/”.

The following fields should be exchanged with the new connection parameters and the new database:

- properties.config

	server=localhost
	database=testcv
	user_db=testCV
	pass_db=testCV 
	host_db=jdbc:mysql://127.0.0.1:3306/?autoReconnect=true 

- propertiesNormalization.config

	host_db=jdbc:mysql://127.0.0.1:3306/?autoReconnect=true
	host_db2=127.0.0.1
	user_db=testCV
	pass_db=testCV

If the process works it generates:

http://localhost:8080/DataPushService/services/DataPushService?wsdl

NOTE: This service contains a stub to CoreDataset Service and MirhtConnect on these urls: http://localhost:8080/CoreDatasetService/services/CoreDatasetService?wsdl
http://localhost:8091/services/MirthConnectWS
